{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMupvzXo40pMAIzFEXFtXs+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# libraries we need\n",
        "import torch # Pytorch - hopefully it's clear why we need this\n",
        "import numpy as np # Numpy - useful for data manipulation.\n",
        "import matplotlib.pyplot as plt # Lets us display data\n",
        "import torchvision # We use this for the dataset we'll use\n",
        "# By importing torch we can already access these - this is just a convenience thing\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from tqdm.notebook import tqdm # Lets us see loop progress"
      ],
      "metadata": {
        "id": "sDiqb-BzLc1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#An essential feauture of pytorch is its ability to utlize GPUs and TPUs\n",
        "#This portion of the code chooses the device that while be used,\n",
        "#if there is a GPU that uses the CUDA toolkit, then it will utlize it\n",
        "#if not it will use the CPU for calculations\n",
        "#Note many times, GPUs are much faster option, and allow for more realistic training times\n",
        "\n",
        "if torch.cuda.is_available(): # Checks if CUDA is availiable, loads the device for computation to the GPU\n",
        "    device = torch.device('cuda:0')\n",
        "    print('Running on GPU')\n",
        "    print(torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print('Running on CPU')"
      ],
      "metadata": {
        "id": "OXYV0zT_LiTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# download the CIFAR 10 training and testing data sets from torchvision.datasets\n",
        "# There are a set of datasets that are stored on server for pytorch, anyone can donwload them if they have pytorch set up\n",
        "# They are split to training and test dataset, which do not intersect!\n",
        "# If you were doing this with your own dataset, you would have to make your own dataset!\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./cifar10', transform=torchvision.transforms.ToTensor(), download=True)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./cifar10', train=False, transform=torchvision.transforms.ToTensor(), download=True)\n",
        "\n",
        "#The dataloader class makes it easy for us to handle and randomize data\n",
        "#The train and test loader both have a 128 sized batches of images, and are shuffled to increase randomization(improves performance)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, 128, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, 128, shuffle=True)\n",
        "\n",
        "# visualizing a sample from train loader\n",
        "print(train_dataset)\n",
        "\n",
        "# Extracting an image and label from the dataset\n",
        "train_iter = iter(train_loader)\n",
        "batch_images, batch_labels = next(train_iter)\n",
        "# The label is a number between 0 and 9 - why might that be?  Isn't the label a category, not a number?\n",
        "# The image is represented as a 3 by 32 by 32 matrix (A 3-dimensional array)\n",
        "image, label = batch_images[0], batch_labels[0]\n",
        "\n",
        "print(image.shape)\n",
        "plt.imshow(image.permute(1, 2, 0)) # Hmmm... this gives us an error.  How might we fix this?  What is causing the error?\n",
        "# HINT: - look into what permute does, and think about the dimensions of the image\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gRlXILZdLpZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_brDNxanlN-"
      },
      "outputs": [],
      "source": [
        "#This is a class module for to create a CNN, not the Module class in pytorch\n",
        "#Is the base class for all models in pytorch, this contains the inner working of a module\n",
        "\n",
        "#Functions ->\n",
        "# The def __init__(self) is a constructor, where you outline the different layers and aspects of your custom class\n",
        "# def forward is the function for forward propogationm you give it an input X and it outputs tensore\n",
        "\n",
        "#Layers ->\n",
        "#In pytorch a nn.Conv2d layer is a convolution 2d layer, the arguments are as follows\n",
        "#nn.Conv2d(Number of Input features maps, Number of features maps, Kernel Size, Stride Size, Padding Size )\n",
        "#nn.BatchNorm2d is a batch normalization layer that takes in a 2d tensor the argument is the number of input feature maps\n",
        "\n",
        "#Hint ->\n",
        "#There is going to be a lot of repitition in your code - it's supposed to look like that, don't worry\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # First convolutional layer\n",
        "        # Here we're defining a standard layer with Convolution, BatchNorm, and dropout\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1, stride=2) # b x 3 x 32 x 32 -> b x 32 x 16 x 16\n",
        "        self.batchnorm1 = nn.BatchNorm2d(32);\n",
        "        self.relu1 = nn.ReLU() # Using ReLU activation function\n",
        "        self.dropout1 = nn.Dropout(0.1)# Adding dropout to prevent overfitting (recommend a rate of 0.1)\n",
        "\n",
        "        # Second convolutional layer\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1, stride=2)# b x 32 x 16 x 16 -> b x 64 x 8 x 8\n",
        "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(2, 2) # Adding a pooling layer to reduce spatial dimensions, b x 64 x 8 x 8 -> b x 64 x 4 x 4\n",
        "        self.dropout2 = nn.Dropout(0.05)# Recommend rate of 0.05\n",
        "\n",
        "        # Third convolutional layer\n",
        "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)# b x 64 x 4 x 4 -> b x 64 x 4 x 4\n",
        "        self.batchnorm3 = nn.BatchNorm2d(64)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout3 = nn.Dropout(0.05) # Recommend rate of 0.05\n",
        "        self.flatten = nn.Flatten()  # b x 64 x 4 x 4 -> b x (64 * 4 * 4)\n",
        "\n",
        "        # Fully connected layer - classifying the features into 10 classes\n",
        "        self.fc = nn.Linear(64 * 4 * 4, 128) # 64 from the last conv layer, 10 for the number of classes, b x (64 * 4 * 4) -> b x 128\n",
        "        self.relu4 =  nn.ReLU()\n",
        "        self.fc1 = nn.Linear(128, 10)  # b x 128 -> b x 10 - left this one in as a hint ;)\n",
        "\n",
        "    # This is already done - we're just calling the functions we define\n",
        "    def forward(self, x):\n",
        "        # Describing the forward pass through the network\n",
        "        x = self.conv1(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        x = self.conv3(x)\n",
        "        x = self.batchnorm3(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.dropout3(x)\n",
        "\n",
        "        # After all those conv layers we can finally pass into a fully connected layer\n",
        "        # Think about it like the neural network does a bunch of pre processing to make the image easier to understand before looking at it\n",
        "        x = self.flatten(x)  # Flattening the output of the conv layers for the fully connected layer\n",
        "        x = self.fc(x)\n",
        "        x = self.relu4(x)\n",
        "        x = self.fc1(x)\n",
        "        return x  # The softmax (or another activation) can be implicitly applied by the loss function"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We are creating an instance of our CNN model, after which we load to model to\n",
        "# the device either GPU or CPU\n",
        "model = CNN()\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "#This is where we define our loss, in this case the loss is cross entorpy\n",
        "#Remember - the loss is a number that tells our model how good it's doing.  The specifics of how this works are a theory track topic\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "#We define the optimizer here, the model.paramters() ar all the paramters of our model, lr is the learning rate\n",
        "#Again, the specifics of how this optimizer works are a theory track topic\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.003, weight_decay=1e-5)"
      ],
      "metadata": {
        "id": "jSf8RsPsMIpE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the training loop, it will take the model, train loader, the optimizer and device\n",
        "#It loops through each training data and trains the model\n",
        "#Note the data is loaded in batches not single instances, this is important\n",
        "def train_one_epoch(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    for i, batch in tqdm(enumerate(train_loader)):  # looping through\n",
        "        inputs, labels = batch # The bacth contains the inputs and labels\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(inputs)#\n",
        "        loss = criterion(outputs, labels) # Error is calculated here, by the criterion(ie cross entropy loss)\n",
        "        loss.backward()  # Computes the gradients and stores it in the model parameters' .grad attribute (this is backprop! or autodiff)\n",
        "        optimizer.step() # Updates the weights to their new value(gradient update)\n",
        "        optimizer.zero_grad()# Zeros out the gradients for next iteration\n",
        "    print('End of epoch loss:', round(loss.item(), 3))"
      ],
      "metadata": {
        "id": "Tkc94MvkMNfQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This is the same as above except that there is no optimization just testing for accruacy\n",
        "@torch.no_grad()\n",
        "def test(model, test_loader, device):\n",
        "    # we've manually specified the classes - these are from the cifar-10 dataset\n",
        "    classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "    # what is the first thing to do before testing?\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    for i, batch in tqdm(enumerate(test_loader)):\n",
        "         inputs, labels = batch\n",
        "         inputs = inputs.to(device)\n",
        "         labels = labels.to(device)\n",
        "         outputs = model(inputs)\n",
        "         predictions = outputs.argmax(dim=1)  # We take the maximum of the predictions, we take the max probability\n",
        "         correct += (predictions == labels).sum().item()\n",
        "\n",
        "    print(f\"End of epoch accuracy: {100 * correct / len(test_dataset)}%\")\n",
        "\n",
        "    # visualizing the current model's performance\n",
        "    for i in range(min(len(inputs), 8)):\n",
        "        print('Guess:', classes[predictions[i]], '| Label:', classes[labels[i]])\n",
        "        plt.imshow(inputs[i].cpu().permute(1,2,0))\n",
        "        plt.show()"
      ],
      "metadata": {
        "id": "Kyd54irDMR60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_EPOCHS = 10 # One epoch is one loop through the training data\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(\"Epoch: \", epoch + 1)\n",
        "    train_one_epoch(model, train_loader, optimizer, criterion, device)\n",
        "    test(model, test_loader, device)"
      ],
      "metadata": {
        "id": "xFCOmje9Lw2L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}